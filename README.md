# DepthEstimationDatasets
Repository providing descriptions of commonly used depth estimation datasets. As will be made available soon, it will also provide a convenient interface to access these datasets. Along with that, we suggest preprocessing operations on the dataset as being proposed in previous work and can be regarded as "standard" for the given dataset. As there is sometimes confusion about how the data for evaluation should look like, we also summarize commonly used schemes.

Feel free to contribute more dataset descriptions and interfaces.

## Dataset Descriptions

### HR-WSI [[1]](https://kexianhust.github.io/Structure-Guided-Ranking-Loss/)

#### Dataset Access

The dataset can be downloaded from [here](https://drive.google.com/file/d/1OVOx6x-B0Cs-m2z_-7ZxSgRFHz_VBvDd/view?usp=sharing) (as referenced in the [official repository](https://github.com/KexianHust/Structure-Guided-Ranking-Loss)).

#### Dataset Properties

##### Meta Data:

| Property  | Value |
| :----- | :---- |
| # Instances (train + test) | 20378 + 3600 |
| Image Resolution | Varying |
| Depth Resolution | Varying (same as corresponding image) |
| Depth Annotation Type | Flow from stereo images (disparity) |
| Dense Annotation? | :heavy_check_mark: |
| Metric Depth? | Pseudo-depth |
| Depth Sensor | FlowNet 2.0 |
| Depth Sensor Capacity | - |
| Additional Annotations | Depth consistency mask, instance mask |
| Diversity | High |

The consistency masks are generated by checking the consistency of flow predictions for interchanged image order (backward-forward). The instance masks are generated by a pretrained Mask-R-CNN model.

##### Scene Types:

| Name | Captured? |
| :--- | :-------- |
| Indoor | :heavy_check_mark: |
| Outdoor | :heavy_check_mark: |


#### Preprocessing

The data is already preprocessed and can be used as provided by the authors. As the flow prediction fails for some pixels, it is recommended to mask out unreasonable depth annotations by the provided consistency masks.

Since the data is given in inverse-depth spacee, one may invert the depth values.

### Sintel [[2]](http://files.is.tue.mpg.de/black/papers/ButlerECCV2012.pdf)

#### Dataset Access

The dataset can be downloaded from [here](http://files.is.tue.mpg.de/jwulff/sintel/MPI-Sintel-depth-training-20150305.zip) (as referenced on the [official project page](http://sintel.is.tue.mpg.de/depth)).

#### Dataset Properties

##### Meta Data:

| Property  | Value |
| :----- | :---- |
| # Instances (train + test) | 1064 |
| Image Resolution | 1024 x 436 |
| Depth Resolution | 1024 x 436 |
| Depth Annotation Type | Synthetic |
| Dense Annotation? | :heavy_check_mark: |
| Metric Depth? | Metric |
| Depth Sensor | Blender |
| Depth Sensor Capacity | - |
| Additional Annotations | Extrinsic and intrinsic camera matrix |
| Diversity | Medium |

##### Scene Types:

| Name | Captured? |
| :--- | :-------- |
| Indoor | :heavy_check_mark: |
| Outdoor | :heavy_check_mark: |

#### Preprocessing

The depth annotation is given both as `.png` and `.dpt` files. For the latter, the authors provide a (Python) SDK along with the depth annotation to parse such files.


### TUM [[3](http://ais.informatik.uni-freiburg.de/publications/papers/sturm12iros.pdf), [4](https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Learning_the_Depths_of_Moving_People_by_Watching_Frozen_People_CVPR_2019_paper.pdf)]

#### Dataset Access

The dataset can be found on the [official project page](https://vision.in.tum.de/data/datasets/rgbd-dataset). However, we recommend to use the preprocessed version as provided in [this repository](https://github.com/google/mannequinchallenge). There, a download script is given to ease the dataset access. This dataset version also features improved (pseudo-)depth annotations constructed by a motion parallax approach. In the following, we list the dataset properties of this third-party preprocessed version.

#### Dataset Properties

##### Meta Data:

| Property  | Value |
| :----- | :---- |
| # Instances (train + test) | 1816 |
| Image Resolution | 384 x 512 |
| Depth Resolution | 384 x 512 |
| Depth Annotation Type | RGB-D, Plane-Plus-Parallax from stereo image flow |
| Dense Annotation? | :heavy_check_mark: |
| Metric Depth? | Metric, pseudo-metric |
| Depth Sensor | Kinect V1, FlowNet 2.0 |
| Depth Sensor Capacity | 10 m, - |
| Additional Annotations | Human mask, intrinsic camera matrix, image keypoints, angle prior, raw flow predictions |
| Diversity | Low |

##### Scene Types:

| Name | Captured? |
| :--- | :-------- |
| Indoor | :heavy_check_mark: |
| Outdoor |  |

#### Preprocessing

For the preprocessed version, no additional means are necessary.

### iBims-1 [[5]](https://openaccess.thecvf.com/content_ECCVW_2018/papers/11131/Koch_Evaluation_of_CNN-based_Single-Image_Depth_Estimation_Methods_ECCVW_2018_paper.pdf)

#### Dataset Access

The dataset can be downloaded from [here](https://dataserv.ub.tum.de/index.php/s/m1455541) (as referenced on the [official project page](https://www.bgu.tum.de/lmf/ibims1/)).

#### Dataset Properties

##### Meta Data:

| Property  | Value |
| :----- | :---- |
| # Instances (train + test) | 100 |
| Image Resolution | 640 x 480 (higher resolution is available, too) |
| Depth Resolution | 640 x 480 |
| Depth Annotation Type | RGB-D |
| Dense Annotation? | :heavy_check_mark: |
| Metric Depth? | Metric |
| Depth Sensor | LiDAR |
| Depth Sensor Capacity | 50 m |
| Additional Annotations | Augmented images, multi-view stereo image pairs |
| Diversity | Low |

##### Scene Types:

| Name | Captured? |
| :--- | :-------- |
| Indoor | :heavy_check_mark: |
| Outdoor |  |

#### Preprocessing

tbd

## References

* [1]: Xian, Ke et al. “Structure-Guided Ranking Loss for Single Image Depth Prediction.” 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020): 608-617.
* [2]: Butler, D. et al. “A Naturalistic Open Source Movie for Optical Flow Evaluation.” ECCV (2012).
* [3]: Sturm, J. et al. “A benchmark for the evaluation of RGB-D SLAM systems.” 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (2012): 573-580.
* [4]: Li, Z. et al. “Learning the Depths of Moving People by Watching Frozen People.” 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019): 4516-4525.
* [5]: Koch, T. et al. “Evaluation of CNN-based Single-Image Depth Estimation Methods.” ECCV Workshops (2018).